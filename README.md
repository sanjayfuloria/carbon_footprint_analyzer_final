    # ğŸŒ± Carbon Footprint Analyzer

**AI-powered carbon footprint estimation from Indian bank statements using LangGraph**

Analyze your spending patterns and estimate carbon emissions with privacy-first PII redaction, high-value transaction filtering, and hybrid rule-based + LLM categorization.

---

## ğŸ“Š Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CARBON FOOTPRINT ANALYSIS PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   PDF    â”‚â”€â”€â”€â–¶â”‚   Extract    â”‚â”€â”€â”€â–¶â”‚   Redact    â”‚â”€â”€â”€â–¶â”‚ Filter High  â”‚   â”‚
â”‚  â”‚  Parser  â”‚    â”‚ Transactions â”‚    â”‚    PII      â”‚    â”‚ Value Txns   â”‚   â”‚
â”‚  â”‚  Node 1  â”‚    â”‚   Node 2     â”‚    â”‚   Node 3    â”‚    â”‚   Node 4     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                â”‚                    â”‚                   â”‚           â”‚
â”‚   PyMuPDF          Groq LLM            Regex-based         â‰¥â‚¹50,000        â”‚
â”‚   Text Extract   llama-3.3-70b         Filtering           Exclusion       â”‚
â”‚                                                                 â”‚           â”‚
â”‚                                                                 â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Rule   â”‚â”€â”€â”€â–¶â”‚     LLM      â”‚â”€â”€â”€â–¶â”‚  Estimate   â”‚â”€â”€â”€â–¶â”‚  Aggregate   â”‚   â”‚
â”‚  â”‚Categorizeâ”‚    â”‚  Categorize  â”‚    â”‚   Carbon    â”‚    â”‚   Results    â”‚   â”‚
â”‚  â”‚  Node 5  â”‚    â”‚   Node 6     â”‚    â”‚   Node 7    â”‚    â”‚   Node 8     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                â”‚                    â”‚                   â”‚           â”‚
â”‚   Pattern          Anthropic/          Emission            Category &       â”‚
â”‚   Matching         Groq LLM            Factors             Monthly Totals   â”‚
â”‚                                                                 â”‚           â”‚
â”‚                                                                 â–¼           â”‚
â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                                        â”‚ Generate    â”‚                      â”‚
â”‚                                        â”‚ Insights    â”‚                      â”‚
â”‚                                        â”‚  Node 9     â”‚                      â”‚
â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                             â”‚                               â”‚
â”‚                                        AI-powered                           â”‚
â”‚                                        Recommendations                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”’ **Privacy-First** | PII redaction (mobile numbers, UPI IDs, account numbers) before LLM processing |
| ğŸ’° **High-Value Filtering** | Excludes transactions â‰¥â‚¹50,000 from spend-based analysis (needs activity-based estimation) |
| âš¡ **Hybrid Efficiency** | Rule-based (80%) + LLM (20%) categorization for speed and cost |
| ğŸ‡®ğŸ‡³ **India-Specific** | Emission factors from NSSO studies, supports all major Indian banks |
| ğŸ“Š **Min/Max Ranges** | Accounts for lifestyle variations (diet, energy sources, etc.) |
| ğŸ“ˆ **Timeline Analysis** | Weekly carbon footprint trends with urban India baseline comparison |
| ğŸ¤– **Multi-LLM** | Supports Anthropic Claude and Groq Llama models |

---

## ğŸ—ï¸ Project Structure

```
carbon_footprint_langgraph/
â”œâ”€â”€ core/                           # Core components
â”‚   â”œâ”€â”€ state.py                   # State definitions
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â””â”€â”€ llm_factory.py             # LLM initialization
â”œâ”€â”€ nodes/                          # 9 Processing nodes
â”‚   â”œâ”€â”€ pdf_parser.py              # Node 1: PDF text extraction
â”‚   â”œâ”€â”€ transaction_extractor.py   # Node 2: LLM transaction extraction
â”‚   â”œâ”€â”€ pii_redactor.py            # Node 3: PII redaction
â”‚   â”œâ”€â”€ high_value_filter.py       # Node 4: High-value transaction filter (NEW)
â”‚   â”œâ”€â”€ rule_categorizer.py        # Node 5: Pattern-based categorization
â”‚   â”œâ”€â”€ llm_categorizer.py         # Node 6: AI categorization
â”‚   â”œâ”€â”€ carbon_estimator.py        # Node 7: Emission calculation
â”‚   â”œâ”€â”€ aggregator.py              # Node 8: Results aggregation
â”‚   â””â”€â”€ insights_generator.py      # Node 9: Recommendations
â”œâ”€â”€ utils/                          # Utilities
â”‚   â”œâ”€â”€ patterns.py                # Categories & emission factors
â”‚   â”œâ”€â”€ sample_data.py             # Demo data
â”‚   â””â”€â”€ reporting.py               # Report generation
â”œâ”€â”€ orchestrator.py                 # LangGraph workflow
â”œâ”€â”€ streamlit_app.py                # Web interface with timeline charts
â”œâ”€â”€ SpendCategory-EmissionFactor... # Source of truth for categories
â””â”€â”€ requirements.txt
```

---

## ğŸš¨ High-Value Transaction Handling

**NEW FEATURE**: Transactions â‰¥â‚¹50,000 are automatically flagged and excluded from spend-based carbon estimation.

### Why Exclude High-Value Transactions?

| Transaction Type | Spend-Based Issue | Recommended Approach |
|------------------|-------------------|---------------------|
| ğŸ’» Electronics (â‚¹50K laptop) | Price â‰  Carbon footprint | Activity-based: ~300-400 kg CO2e |
| âœˆï¸ International flights | Ticket price varies by booking time | Activity-based: Distance Ã— emission factor |
| ğŸ  Property/Vehicles | Lifecycle emissions unrelated to price | Activity-based: Product-specific factors |
| ğŸ’° Investments/Insurance | No direct carbon emissions | Exclude from carbon analysis |

### How It Works

1. **Node 4** filters transactions before categorization
2. **High-value transactions** are flagged but not categorized
3. **Carbon estimation** only applies to regular transactions
4. **Results display** shows excluded transactions with recommendations

---

## ğŸ“ˆ Timeline Analysis & Benchmarking

**NEW FEATURE**: Weekly carbon footprint trends with urban India baseline comparison.

### Timeline Chart Features

- ğŸ“Š **Weekly aggregation** of carbon emissions
- ğŸ“ˆ **Min/Max ranges** showing lifestyle variation
- ğŸ”¶ **Urban India baseline** (dotted orange line at ~8.5 kg CO2e/week)
- ğŸ“‹ **Comparison metrics** (above/below average)
- ğŸ¯ **Trend analysis** (increasing/decreasing over time)

### Reference Context

| Benchmark | Weekly CO2e | Annual CO2e | Source |
|-----------|-------------|-------------|---------|
| **Urban India Average** | ~8.5 kg | ~450 kg | NSSO consumption studies |
| **National India Average** | ~4.6 kg | ~240 kg | Per capita emissions data |
| **Global Average** | ~9.6 kg | ~500 kg | World Bank data |

---

## ğŸ“Š Sample Output

```
ğŸŒ± CARBON FOOTPRINT ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Summary
â”œâ”€ Total Footprint: 158.45 - 316.90 kg CO2e
â”œâ”€ Average Estimate: 237.68 kg CO2e
â”œâ”€ Transactions Analyzed: 42 debits
â”œâ”€ High-Value Excluded: 3 transactions (â‚¹2,15,000)
â””â”€ Trees to Offset: 11.3 trees/year

ğŸ·ï¸ Categorization Efficiency
â”œâ”€ Rule-based: 34 transactions (81%)
â”œâ”€ LLM-based: 8 transactions (19%)
â””â”€ Processing Time: 8.2 seconds

ğŸ“ˆ Timeline Analysis
â”œâ”€ Your Weekly Average: 9.2 kg CO2e
â”œâ”€ vs Urban India: 0.7 kg above average
â”œâ”€ Trend: Decreasing over time
â””â”€ Highest Week: 15.3 kg CO2e

ğŸ“ˆ Top Categories
â”œâ”€ ğŸš— Transport: 42% (99.8 kg CO2e)
â”œâ”€ ğŸ  Housing: 28% (66.6 kg CO2e)
â”œâ”€ ğŸ½ï¸ Food: 18% (42.8 kg CO2e)
â””â”€ ğŸ›ï¸ Shopping: 12% (28.5 kg CO2e)

âš ï¸ High-Value Transactions (Activity-Based Needed)
â”œâ”€ Electronics Purchase: â‚¹85,000
â”œâ”€ International Flight: â‚¹75,000
â””â”€ Property Investment: â‚¹55,000

ğŸ’¡ Recommendations
â”œâ”€ Consider carpooling or public transport
â”œâ”€ Switch to LED bulbs and energy-efficient appliances
â”œâ”€ Reduce food delivery, cook more at home
â””â”€ For high-value items, use activity-based carbon calculators
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone and setup
git clone <repo-url>
cd carbon_footprint_langgraph

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env and add your GROQ_API_KEY and/or ANTHROPIC_API_KEY
```

### 2. Run the Application

```bash
# Web interface (recommended)
streamlit run streamlit_app.py

# Or use the orchestrator directly
python orchestrator.py
```

### 3. Analyze Your Statement

1. Open http://localhost:8501
2. Upload your bank statement PDF
3. Select LLM provider (Groq recommended for extraction)
4. Click "Analyze Carbon Footprint"
5. View results, timeline charts, and recommendations
6. Check high-value transaction alerts for activity-based estimation

---

## ğŸ“ˆ How It Works

### Node-by-Node Flow (Updated)

```mermaid
graph LR
    A[PDF Upload] --> B[Node 1: Parse PDF]
    B --> C[Node 2: Extract Transactions]
    C --> D[Node 3: Redact PII]
    D --> E[Node 4: Filter High-Value]
    E --> F[Node 5: Rule Categorize]
    F --> G[Node 6: LLM Categorize]
    G --> H[Node 7: Estimate Carbon]
    H --> I[Node 8: Aggregate]
    I --> J[Node 9: Generate Insights]
    J --> K[Timeline Charts & Results]
    
    E --> L[High-Value Transactions]
    L --> M[Activity-Based Recommendation]
    
    style E fill:#ffeb3b
    style L fill:#ff5722
    style M fill:#ff5722
```

### Detailed Node Descriptions (Updated)

| Node | Input | Processing | Output |
|------|-------|------------|--------|
| **1. PDF Parser** | PDF file | PyMuPDF text extraction | Raw text |
| **2. Transaction Extractor** | Raw text | Groq LLM parsing | Structured transactions |
| **3. PII Redactor** | Transactions | Regex pattern matching | Redacted transactions |
| **4. High-Value Filter** | Redacted txns | â‰¥â‚¹50K threshold check | Regular + high-value splits |
| **5. Rule Categorizer** | Regular txns | 200+ merchant patterns | Categorized (80%) |
| **6. LLM Categorizer** | Uncategorized | AI classification | Fully categorized |
| **7. Carbon Estimator** | Categories | Emission factors Ã— amount | CO2e estimates |
| **8. Aggregator** | Estimates | Sum by category/month/week | Totals & breakdowns |
| **9. Insights Generator** | Aggregated | AI analysis + timeline | Recommendations |

---

## ğŸ”’ Privacy & Compliance

### PII Redaction (DPDP Act 2023)

The system automatically redacts:
- ğŸ“± Mobile numbers (10-digit patterns)
- ğŸ’³ UPI IDs (name@bank patterns)
- ğŸ¦ Account numbers (8-18 digit patterns)
- ğŸ“§ Email addresses
- ğŸ†” PAN/Aadhaar patterns

**Only redacted descriptions are sent to LLMs** - sensitive data stays local.

### High-Value Transaction Privacy

- High-value transactions (â‰¥â‚¹50,000) are **never sent to LLMs**
- Only transaction amount and truncated description stored locally
- Full transaction details remain in your local analysis only

---

## ğŸ› ï¸ Development

### Configuring High-Value Threshold

Edit `nodes/high_value_filter.py`:

```python
# Adjust threshold as needed
HIGH_VALUE_THRESHOLD = 50000  # â‚¹50,000 (default)
```

### Adding Timeline Benchmarks

Edit `streamlit_app.py`:

```python
# Add new benchmark lines
urban_avg_weekly = 8.5  # kg CO2e per week
rural_avg_weekly = 4.0  # kg CO2e per week (example)
```

### Customizing Emission Factors

Edit `utils/patterns.py`:

```python
EMISSION_FACTORS = {
    "transport": {
        "min": 20, "max": 40,  # Adjust these values
        "source": "Your source",
        "notes": "Your notes"
    },
    # ...
}
```

---

## ğŸ“‹ Requirements

- Python 3.10+
- Groq API key (required for transaction extraction)
- Anthropic API key (optional, for categorization)

### Key Dependencies

```
langchain>=0.3.0
langgraph>=0.2.0
langchain-anthropic>=0.3.0
langchain-groq>=0.2.0
streamlit>=1.40.0
pymupdf>=1.24.0
plotly>=5.0.0
pandas>=2.0.0
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with sample data (including high-value transactions)
5. Submit a pull request

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **LangGraph** - Workflow orchestration
- **Anthropic & Groq** - LLM providers
- **NSSO** - Emission factor data
- **Indian Banking Standards** - Statement formats
- **Urban India Carbon Studies** - Baseline benchmarking data

---

**ğŸŒ± Start tracking your carbon footprint today with intelligent high-value filtering and timeline analysis!**

```bash
streamlit run streamlit_app.py
```

